{"cells":[{"cell_type":"code","execution_count":198,"metadata":{"executionInfo":{"elapsed":9023,"status":"ok","timestamp":1655587426734,"user":{"displayName":"Ifeanyi Akawi","userId":"10822053801368667109"},"user_tz":-60},"id":"KiLRpLEfmxqM"},"outputs":[],"source":["# import necessary packages\n","import transformers\n","import pandas as pd\n","from datasets import Dataset, load_metric\n","import numpy as np\n","from torch.nn import CrossEntropyLoss\n","from transformers import AutoTokenizer, DataCollatorWithPadding, TrainingArguments, AutoModelForSequenceClassification, Trainer"]},{"cell_type":"markdown","metadata":{"id":"ZaF59TvWsQI8"},"source":["### Prepare the Dataset\n"]},{"cell_type":"code","execution_count":199,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":394},"executionInfo":{"elapsed":418,"status":"ok","timestamp":1655587468013,"user":{"displayName":"Ifeanyi Akawi","userId":"10822053801368667109"},"user_tz":-60},"id":"nuQB_BJhoLRl","outputId":"e3890f25-acb9-42e7-a0f1-2192b1739298"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Email Text</th>\n","      <th>Category</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Thank you for your help.</td>\n","      <td>Thank You</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>I appreciate your assistance.</td>\n","      <td>Thank You</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Grateful for your support.</td>\n","      <td>Thank You</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Thanks a lot!</td>\n","      <td>Thank You</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>I'm going through a tough time. Can you help?</td>\n","      <td>Other</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>I need assistance with a difficult situation.</td>\n","      <td>Other</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>Please, I'm in a crisis. Can you assist me?</td>\n","      <td>Other</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>I'll think about it and get back to you.</td>\n","      <td>Other</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>Hello, how are you doing?</td>\n","      <td>Other</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>I'm struggling right now. Can you help?</td>\n","      <td>Other</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>Sorry, I can't help you with this.</td>\n","      <td>Other</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>Can we discuss this further?</td>\n","      <td>Other</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>Thank you for your prompt response.</td>\n","      <td>Thank You</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>I'm really grateful for your support.</td>\n","      <td>Thank You</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>Thanks for your assistance.</td>\n","      <td>Thank You</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>I'm facing a challenging issue. Can you assist?</td>\n","      <td>Other</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>Urgent help needed!</td>\n","      <td>Other</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>Please provide more details.</td>\n","      <td>Other</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>Let's continue the conversation.</td>\n","      <td>Other</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>Hello there!</td>\n","      <td>Other</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>I'm no longer interested.</td>\n","      <td>Other</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>I found another solution.</td>\n","      <td>Other</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>Not looking for help right now.</td>\n","      <td>Other</td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>Sorry, I can't assist with this.</td>\n","      <td>Other</td>\n","    </tr>\n","    <tr>\n","      <th>24</th>\n","      <td>Can we explore this further?</td>\n","      <td>Other</td>\n","    </tr>\n","    <tr>\n","      <th>25</th>\n","      <td>Thanks a million!</td>\n","      <td>Thank You</td>\n","    </tr>\n","    <tr>\n","      <th>26</th>\n","      <td>Your help means a lot.</td>\n","      <td>Thank You</td>\n","    </tr>\n","    <tr>\n","      <th>27</th>\n","      <td>Appreciate your time and effort.</td>\n","      <td>Thank You</td>\n","    </tr>\n","    <tr>\n","      <th>28</th>\n","      <td>I need more information.</td>\n","      <td>Other</td>\n","    </tr>\n","    <tr>\n","      <th>29</th>\n","      <td>Urgent matter, please respond.</td>\n","      <td>Other</td>\n","    </tr>\n","    <tr>\n","      <th>30</th>\n","      <td>Please respond to my inquiry.</td>\n","      <td>Other</td>\n","    </tr>\n","    <tr>\n","      <th>31</th>\n","      <td>Hi, how are you today?</td>\n","      <td>Other</td>\n","    </tr>\n","    <tr>\n","      <th>32</th>\n","      <td>I'm not interested at this time.</td>\n","      <td>Other</td>\n","    </tr>\n","    <tr>\n","      <th>33</th>\n","      <td>I've found another way.</td>\n","      <td>Other</td>\n","    </tr>\n","    <tr>\n","      <th>34</th>\n","      <td>I'll get back to you later.</td>\n","      <td>Other</td>\n","    </tr>\n","    <tr>\n","      <th>35</th>\n","      <td>Thanks for everything.</td>\n","      <td>Thank You</td>\n","    </tr>\n","    <tr>\n","      <th>36</th>\n","      <td>Your support is invaluable.</td>\n","      <td>Thank You</td>\n","    </tr>\n","    <tr>\n","      <th>37</th>\n","      <td>I can't comprehend this, can you help?</td>\n","      <td>Other</td>\n","    </tr>\n","    <tr>\n","      <th>38</th>\n","      <td>Urgent request for assistance.</td>\n","      <td>Other</td>\n","    </tr>\n","    <tr>\n","      <th>39</th>\n","      <td>Please shed light on this.</td>\n","      <td>Other</td>\n","    </tr>\n","    <tr>\n","      <th>40</th>\n","      <td>Let's discuss this further.</td>\n","      <td>Other</td>\n","    </tr>\n","    <tr>\n","      <th>41</th>\n","      <td>Hello, what's going on?</td>\n","      <td>Other</td>\n","    </tr>\n","    <tr>\n","      <th>42</th>\n","      <td>I'm no longer interested in this.</td>\n","      <td>Other</td>\n","    </tr>\n","    <tr>\n","      <th>43</th>\n","      <td>I've sorted it out myself.</td>\n","      <td>Other</td>\n","    </tr>\n","    <tr>\n","      <th>44</th>\n","      <td>Not seeking help currently.</td>\n","      <td>Other</td>\n","    </tr>\n","    <tr>\n","      <th>45</th>\n","      <td>Sorry, can't provide assistance.</td>\n","      <td>Other</td>\n","    </tr>\n","    <tr>\n","      <th>46</th>\n","      <td>Can we delve into this further?</td>\n","      <td>Other</td>\n","    </tr>\n","    <tr>\n","      <th>47</th>\n","      <td>Thank you for your kindness.</td>\n","      <td>Thank You</td>\n","    </tr>\n","    <tr>\n","      <th>48</th>\n","      <td>Your assistance is greatly appreciated.</td>\n","      <td>Thank You</td>\n","    </tr>\n","    <tr>\n","      <th>49</th>\n","      <td>Thanks for being so helpful.</td>\n","      <td>Thank You</td>\n","    </tr>\n","    <tr>\n","      <th>50</th>\n","      <td>I'm puzzled, can you clarify?</td>\n","      <td>Other</td>\n","    </tr>\n","    <tr>\n","      <th>51</th>\n","      <td>Urgent issue, please respond.</td>\n","      <td>Other</td>\n","    </tr>\n","    <tr>\n","      <th>52</th>\n","      <td>Please clarify your previous message.</td>\n","      <td>Other</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                         Email Text   Category\n","0                          Thank you for your help.  Thank You\n","1                     I appreciate your assistance.  Thank You\n","2                        Grateful for your support.  Thank You\n","3                                     Thanks a lot!  Thank You\n","4     I'm going through a tough time. Can you help?      Other\n","5     I need assistance with a difficult situation.      Other\n","6       Please, I'm in a crisis. Can you assist me?      Other\n","7          I'll think about it and get back to you.      Other\n","8                         Hello, how are you doing?      Other\n","9           I'm struggling right now. Can you help?      Other\n","10               Sorry, I can't help you with this.      Other\n","11                     Can we discuss this further?      Other\n","12              Thank you for your prompt response.  Thank You\n","13            I'm really grateful for your support.  Thank You\n","14                      Thanks for your assistance.  Thank You\n","15  I'm facing a challenging issue. Can you assist?      Other\n","16                              Urgent help needed!      Other\n","17                     Please provide more details.      Other\n","18                 Let's continue the conversation.      Other\n","19                                     Hello there!      Other\n","20                        I'm no longer interested.      Other\n","21                        I found another solution.      Other\n","22                  Not looking for help right now.      Other\n","23                 Sorry, I can't assist with this.      Other\n","24                     Can we explore this further?      Other\n","25                                Thanks a million!  Thank You\n","26                           Your help means a lot.  Thank You\n","27                 Appreciate your time and effort.  Thank You\n","28                         I need more information.      Other\n","29                   Urgent matter, please respond.      Other\n","30                    Please respond to my inquiry.      Other\n","31                           Hi, how are you today?      Other\n","32                 I'm not interested at this time.      Other\n","33                          I've found another way.      Other\n","34                      I'll get back to you later.      Other\n","35                           Thanks for everything.  Thank You\n","36                      Your support is invaluable.  Thank You\n","37           I can't comprehend this, can you help?      Other\n","38                   Urgent request for assistance.      Other\n","39                       Please shed light on this.      Other\n","40                      Let's discuss this further.      Other\n","41                          Hello, what's going on?      Other\n","42                I'm no longer interested in this.      Other\n","43                       I've sorted it out myself.      Other\n","44                      Not seeking help currently.      Other\n","45                 Sorry, can't provide assistance.      Other\n","46                  Can we delve into this further?      Other\n","47                     Thank you for your kindness.  Thank You\n","48          Your assistance is greatly appreciated.  Thank You\n","49                     Thanks for being so helpful.  Thank You\n","50                    I'm puzzled, can you clarify?      Other\n","51                    Urgent issue, please respond.      Other\n","52            Please clarify your previous message.      Other"]},"execution_count":199,"metadata":{},"output_type":"execute_result"}],"source":["# load dataset as pandas Dataframe\n","df = pd.read_csv(\"Training Data.csv\")\n","\n","# check\n","df"]},{"cell_type":"code","execution_count":200,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":324,"status":"ok","timestamp":1655587493109,"user":{"displayName":"Ifeanyi Akawi","userId":"10822053801368667109"},"user_tz":-60},"id":"19mTVz2rrrqP","outputId":"f447a33a-c8e4-455e-d645-d1f9c36e0d2d"},"outputs":[{"data":{"text/plain":["datasets.arrow_dataset.Dataset"]},"execution_count":200,"metadata":{},"output_type":"execute_result"}],"source":["# load the dataframe in a hugging face compatible format\n","dataset = Dataset.from_pandas(df)\n","\n","# check the type\n","type(dataset)"]},{"cell_type":"markdown","metadata":{"id":"AbFltOF2uvIk"},"source":["### Preprocessing the dataset\n"]},{"cell_type":"code","execution_count":201,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":118,"referenced_widgets":["0d8302ca67134193a10ade69c44041bf","89e96f9fcfed418895ef4887072b134d","c6b02c2d616f41af9ded54c7916709ee","b88ce659f6f4451eb43bf7bd6610f0d6","30b853bfac2d4eafafa5208e914aa40e","5fe39bf5c8d446dc8dc9aa86429b7423","bf060a5ee51b458aa6517a3bf503e59b","a2505a274ff243589f05bf088c1e0992","36c4eb53fb1f40f9bd232c59979d52db","f9214a7b86334b92b3df3f3389cf154e","7202ebce10d943f5885631fc36418058","f954762c305c4be9ae3fd2682e7efe53","459c48905efe477cb25c5eb8dc3c0571","27549b8720c741199e42b55049ac1c1f","10ef2c11a1754e708834372100e6eb5b","b7032e7ea083403d9abf038b48c328e5","fe8e0db9a7ce4810974b58c0f7fb9ff4","840f26b2aa4640b4919625d9504df3f6","823582762af34fa3b85ab758fc06d8a3","df7418fee5f34b0584500f63aaa480eb","c1cf39a62d064404920f5dbc2175829e","d1d37f46d21e476b80319fb3143055ff"]},"executionInfo":{"elapsed":72,"status":"ok","timestamp":1655570875322,"user":{"displayName":"Ifeanyi Akawi","userId":"10822053801368667109"},"user_tz":-60},"id":"dNNz2rv70Yfj","outputId":"339a52c1-ebd4-4a4c-82f6-c101a7213eb7"},"outputs":[{"name":"stderr","output_type":"stream","text":["Casting to class labels: 100%|██████████| 53/53 [00:00<?, ? examples/s]\n"]}],"source":["# encode the dataset labels as integers\n","dataset = dataset.class_encode_column('Category')"]},{"cell_type":"code","execution_count":202,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":55,"status":"ok","timestamp":1655570875323,"user":{"displayName":"Ifeanyi Akawi","userId":"10822053801368667109"},"user_tz":-60},"id":"cH03nZ8OqJOx","outputId":"182d1a02-e701-4e42-bb32-5d19048df666"},"outputs":[{"data":{"text/plain":["{'Email Text': 'Grateful for your support.', 'Category': 1}"]},"execution_count":202,"metadata":{},"output_type":"execute_result"}],"source":["# view a sample of the dataset\n","dataset[2]"]},{"cell_type":"markdown","metadata":{"id":"XH34IgbA-sd3"},"source":["Within Category, 1 is Thank You, 0 is Other\n"]},{"cell_type":"code","execution_count":203,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":52,"status":"ok","timestamp":1655570875324,"user":{"displayName":"Ifeanyi Akawi","userId":"10822053801368667109"},"user_tz":-60},"id":"_0RBElLCxqnZ","outputId":"4ec48573-84b6-46fa-bbee-bb737aaa8b81"},"outputs":[{"data":{"text/plain":["{'Email Text': Value(dtype='string', id=None),\n"," 'Category': ClassLabel(names=['Other', 'Thank You'], id=None)}"]},"execution_count":203,"metadata":{},"output_type":"execute_result"}],"source":["# verify the dataset features\n","dataset.features"]},{"cell_type":"markdown","metadata":{"id":"nRpuYrIwNF6-"},"source":["### Tokenization\n"]},{"cell_type":"code","execution_count":204,"metadata":{"executionInfo":{"elapsed":1370,"status":"ok","timestamp":1655570876647,"user":{"displayName":"Ifeanyi Akawi","userId":"10822053801368667109"},"user_tz":-60},"id":"J-fs0RpU0BAU"},"outputs":[],"source":["# declare the checkpoint\n","checkpoint = \"bert-base-uncased\"\n","\n","# call the tokenizer for training\n","tokenizer = AutoTokenizer.from_pretrained(checkpoint)"]},{"cell_type":"code","execution_count":205,"metadata":{"executionInfo":{"elapsed":85,"status":"ok","timestamp":1655570876656,"user":{"displayName":"Ifeanyi Akawi","userId":"10822053801368667109"},"user_tz":-60},"id":"wqsctMTmlB22"},"outputs":[],"source":["# create a function for tokenizing the sample_mails\n","def tokenize_function(example):\n","    return tokenizer(example[\"Email Text\"], truncation=True)"]},{"cell_type":"code","execution_count":206,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":118,"referenced_widgets":["fa5f86eee50443969a0cd33309bd7127","faed0052fe0c461095f078464b9dc55c","c72399c3c7ab4781b423a60ba393b5e3","4a4884cabad94a83b0fac2c12b0574e5","df6ea8d5f4764b738c8539fdbf5f47cb","f1b50327eed54763b244185ab211cacf","00d76b58b434493abaea93735bd876d8","19bf2c0fa8c347ec84635386dba59ab5","a39c3c5ee7d24900a6c2fe359c4261d2","3c095e7707b04738952a5b4483b496b5","24c73ce427214973bde836688b2700c7"]},"executionInfo":{"elapsed":89,"status":"ok","timestamp":1655570876666,"user":{"displayName":"Ifeanyi Akawi","userId":"10822053801368667109"},"user_tz":-60},"id":"agGQC2dnPxNU","outputId":"d9631022-a6bd-48c6-b8df-d102b8a2c843"},"outputs":[{"name":"stderr","output_type":"stream","text":["Map: 100%|██████████| 53/53 [00:00<00:00, 2136.89 examples/s]\n"]},{"data":{"text/plain":["Dataset({\n","    features: ['Email Text', 'Category', 'input_ids', 'token_type_ids', 'attention_mask'],\n","    num_rows: 53\n","})"]},"execution_count":206,"metadata":{},"output_type":"execute_result"}],"source":["# tokenize the dataset with the map function\n","tokenized_datasets = dataset.map(tokenize_function, batched=True)\n","tokenized_datasets"]},{"cell_type":"code","execution_count":207,"metadata":{"executionInfo":{"elapsed":78,"status":"ok","timestamp":1655570876671,"user":{"displayName":"Ifeanyi Akawi","userId":"10822053801368667109"},"user_tz":-60},"id":"KBRzYe8fnE6h"},"outputs":[],"source":["# apply dynamic padding -- pad all the sample_mails to the length of the longest element when we batch elements together\n","data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"]},{"cell_type":"markdown","metadata":{"id":"NqjYk8U2nx1L"},"source":["To test this new toy, we'll slice our dataset that we would like to batch together. Here, we remove the columns idx and sample_mails as they won’t be needed and contain strings (and we can’t create tensors with strings) and have a look at the lengths of each entry in the batch:\n"]},{"cell_type":"code","execution_count":208,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":75,"status":"ok","timestamp":1655570876672,"user":{"displayName":"Ifeanyi Akawi","userId":"10822053801368667109"},"user_tz":-60},"id":"80WVTJEynF6D","outputId":"0eab2d28-f044-45d2-a81a-46e3ea3ea189"},"outputs":[],"source":["# samples = tokenized_datasets[:]\n","# samples = {k: v for k, v in samples.items() if k not in [\"idx\", \"Email Text\"]}\n","# [len(x) for x in samples[\"input_ids\"]]\n","fields_to_remove = [\"Email Text\"]\n","\n","# Remove the specified fields\n","tokenized_datasets = tokenized_datasets.remove_columns(fields_to_remove)"]},{"cell_type":"markdown","metadata":{"id":"nOXm9aNApFMv"},"source":["No surprise, we get samples of varying length, from 7 to 15. Dynamic padding means the samples in this batch should all be padded to a length of 15, the maximum length inside the batch. Without dynamic padding, all of the samples would have to be padded to the maximum length in the whole dataset, or the maximum length the model can accept. Let’s double-check that our data_collator is dynamically padding the batch properly:\n"]},{"cell_type":"code","execution_count":209,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":49,"status":"ok","timestamp":1655570876680,"user":{"displayName":"Ifeanyi Akawi","userId":"10822053801368667109"},"user_tz":-60},"id":"_lf3AFOQPyC_","outputId":"7fd91539-1f64-4510-9ffa-5b22a67c19e2"},"outputs":[{"data":{"text/plain":["['[CLS]', 'please', 'clarify', 'your', 'previous', 'message', '.', '[SEP]']"]},"execution_count":209,"metadata":{},"output_type":"execute_result"}],"source":["# we can convert the tokenized dataset back to text as follows\n","tokenizer.convert_ids_to_tokens(tokenized_datasets['input_ids'][-1])"]},{"cell_type":"markdown","metadata":{"id":"Bq1iZSRTZjRC"},"source":["### Training\n","\n","The first step before we can define our Trainer is to define a TrainingArguments class that will contain all the hyperparameters the Trainer will use for training and evaluation. The only argument you have to provide is a directory where the trained model will be saved, in our case we want to also modify the number of epochs for training, the checkpoints along the way are also saved in this directory. For all the rest, you can leave the defaults, which should work pretty well for a basic fine-tuning.\n"]},{"cell_type":"code","execution_count":210,"metadata":{"executionInfo":{"elapsed":453,"status":"ok","timestamp":1655570877090,"user":{"displayName":"Ifeanyi Akawi","userId":"10822053801368667109"},"user_tz":-60},"id":"FK6WKUMWt5gX"},"outputs":[],"source":["# define a metric to monitor during training\n","metric = load_metric(\"accuracy\")\n","\n","# create a function that helps compute the specified metric\n","\n","\n","def compute_metrics(eval_pred):\n","    logits, labels = eval_pred\n","    predictions = np.argmax(logits, axis=-1)\n","    return metric.compute(predictions=predictions, references=labels)"]},{"cell_type":"code","execution_count":211,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1655570877091,"user":{"displayName":"Ifeanyi Akawi","userId":"10822053801368667109"},"user_tz":-60},"id":"cqIKniIhUtgk"},"outputs":[],"source":["# define the training arguments\n","training_args = TrainingArguments('training_args',\n","                                  num_train_epochs=20)"]},{"cell_type":"markdown","metadata":{"id":"tQgo27OMac5X"},"source":["The second step is to define our model. We will use the AutoModelForSequenceClassification class, with two labels:\n"]},{"cell_type":"code","execution_count":212,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3789,"status":"ok","timestamp":1655570880876,"user":{"displayName":"Ifeanyi Akawi","userId":"10822053801368667109"},"user_tz":-60},"id":"k2Qv5h4CacDc","outputId":"548e794a-b9ae-4b7a-b23b-cf6de50c7080"},"outputs":[{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[1;32mc:\\Users\\Ben\\Documents\\GitHub\\Email-classification-with-bert\\train_model.ipynb Cell 24\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Ben/Documents/GitHub/Email-classification-with-bert/train_model.ipynb#X40sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model \u001b[39m=\u001b[39m AutoModelForSequenceClassification\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Ben/Documents/GitHub/Email-classification-with-bert/train_model.ipynb#X40sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     checkpoint, num_labels\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m)\n","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\models\\auto\\auto_factory.py:565\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    563\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mtype\u001b[39m(config) \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys():\n\u001b[0;32m    564\u001b[0m     model_class \u001b[39m=\u001b[39m _get_model_class(config, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping)\n\u001b[1;32m--> 565\u001b[0m     \u001b[39mreturn\u001b[39;00m model_class\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[0;32m    566\u001b[0m         pretrained_model_name_or_path, \u001b[39m*\u001b[39;49mmodel_args, config\u001b[39m=\u001b[39;49mconfig, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhub_kwargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[0;32m    567\u001b[0m     )\n\u001b[0;32m    568\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    569\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized configuration class \u001b[39m\u001b[39m{\u001b[39;00mconfig\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m for this kind of AutoModel: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    570\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel type should be one of \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(c\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m \u001b[39m\u001b[39mfor\u001b[39;00m\u001b[39m \u001b[39mc\u001b[39m \u001b[39m\u001b[39min\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    571\u001b[0m )\n","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\modeling_utils.py:3307\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   3297\u001b[0m     \u001b[39mif\u001b[39;00m dtype_orig \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   3298\u001b[0m         torch\u001b[39m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[0;32m   3300\u001b[0m     (\n\u001b[0;32m   3301\u001b[0m         model,\n\u001b[0;32m   3302\u001b[0m         missing_keys,\n\u001b[0;32m   3303\u001b[0m         unexpected_keys,\n\u001b[0;32m   3304\u001b[0m         mismatched_keys,\n\u001b[0;32m   3305\u001b[0m         offload_index,\n\u001b[0;32m   3306\u001b[0m         error_msgs,\n\u001b[1;32m-> 3307\u001b[0m     ) \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_load_pretrained_model(\n\u001b[0;32m   3308\u001b[0m         model,\n\u001b[0;32m   3309\u001b[0m         state_dict,\n\u001b[0;32m   3310\u001b[0m         loaded_state_dict_keys,  \u001b[39m# XXX: rename?\u001b[39;49;00m\n\u001b[0;32m   3311\u001b[0m         resolved_archive_file,\n\u001b[0;32m   3312\u001b[0m         pretrained_model_name_or_path,\n\u001b[0;32m   3313\u001b[0m         ignore_mismatched_sizes\u001b[39m=\u001b[39;49mignore_mismatched_sizes,\n\u001b[0;32m   3314\u001b[0m         sharded_metadata\u001b[39m=\u001b[39;49msharded_metadata,\n\u001b[0;32m   3315\u001b[0m         _fast_init\u001b[39m=\u001b[39;49m_fast_init,\n\u001b[0;32m   3316\u001b[0m         low_cpu_mem_usage\u001b[39m=\u001b[39;49mlow_cpu_mem_usage,\n\u001b[0;32m   3317\u001b[0m         device_map\u001b[39m=\u001b[39;49mdevice_map,\n\u001b[0;32m   3318\u001b[0m         offload_folder\u001b[39m=\u001b[39;49moffload_folder,\n\u001b[0;32m   3319\u001b[0m         offload_state_dict\u001b[39m=\u001b[39;49moffload_state_dict,\n\u001b[0;32m   3320\u001b[0m         dtype\u001b[39m=\u001b[39;49mtorch_dtype,\n\u001b[0;32m   3321\u001b[0m         is_quantized\u001b[39m=\u001b[39;49m(\u001b[39mgetattr\u001b[39;49m(model, \u001b[39m\"\u001b[39;49m\u001b[39mquantization_method\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m) \u001b[39m==\u001b[39;49m QuantizationMethod\u001b[39m.\u001b[39;49mBITS_AND_BYTES),\n\u001b[0;32m   3322\u001b[0m         keep_in_fp32_modules\u001b[39m=\u001b[39;49mkeep_in_fp32_modules,\n\u001b[0;32m   3323\u001b[0m     )\n\u001b[0;32m   3325\u001b[0m model\u001b[39m.\u001b[39mis_loaded_in_4bit \u001b[39m=\u001b[39m load_in_4bit\n\u001b[0;32m   3326\u001b[0m model\u001b[39m.\u001b[39mis_loaded_in_8bit \u001b[39m=\u001b[39m load_in_8bit\n","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\modeling_utils.py:3649\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[1;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, is_quantized, keep_in_fp32_modules)\u001b[0m\n\u001b[0;32m   3639\u001b[0m \u001b[39mif\u001b[39;00m state_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   3640\u001b[0m     \u001b[39m# Whole checkpoint\u001b[39;00m\n\u001b[0;32m   3641\u001b[0m     mismatched_keys \u001b[39m=\u001b[39m _find_mismatched_keys(\n\u001b[0;32m   3642\u001b[0m         state_dict,\n\u001b[0;32m   3643\u001b[0m         model_state_dict,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3647\u001b[0m         ignore_mismatched_sizes,\n\u001b[0;32m   3648\u001b[0m     )\n\u001b[1;32m-> 3649\u001b[0m     error_msgs \u001b[39m=\u001b[39m _load_state_dict_into_model(model_to_load, state_dict, start_prefix)\n\u001b[0;32m   3650\u001b[0m     offload_index \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   3651\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   3652\u001b[0m     \u001b[39m# Sharded checkpoint or whole but low_cpu_mem_usage==True\u001b[39;00m\n\u001b[0;32m   3653\u001b[0m \n\u001b[0;32m   3654\u001b[0m     \u001b[39m# This should always be a list but, just to be sure.\u001b[39;00m\n","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\modeling_utils.py:571\u001b[0m, in \u001b[0;36m_load_state_dict_into_model\u001b[1;34m(model_to_load, state_dict, start_prefix)\u001b[0m\n\u001b[0;32m    568\u001b[0m         \u001b[39mif\u001b[39;00m child \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    569\u001b[0m             load(child, state_dict, prefix \u001b[39m+\u001b[39m name \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 571\u001b[0m load(model_to_load, state_dict, prefix\u001b[39m=\u001b[39;49mstart_prefix)\n\u001b[0;32m    572\u001b[0m \u001b[39m# Delete `state_dict` so it could be collected by GC earlier. Note that `state_dict` is a copy of the argument, so\u001b[39;00m\n\u001b[0;32m    573\u001b[0m \u001b[39m# it's safe to delete it.\u001b[39;00m\n\u001b[0;32m    574\u001b[0m \u001b[39mdel\u001b[39;00m state_dict\n","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\modeling_utils.py:569\u001b[0m, in \u001b[0;36m_load_state_dict_into_model.<locals>.load\u001b[1;34m(module, state_dict, prefix)\u001b[0m\n\u001b[0;32m    567\u001b[0m \u001b[39mfor\u001b[39;00m name, child \u001b[39min\u001b[39;00m module\u001b[39m.\u001b[39m_modules\u001b[39m.\u001b[39mitems():\n\u001b[0;32m    568\u001b[0m     \u001b[39mif\u001b[39;00m child \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 569\u001b[0m         load(child, state_dict, prefix \u001b[39m+\u001b[39;49m name \u001b[39m+\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\modeling_utils.py:569\u001b[0m, in \u001b[0;36m_load_state_dict_into_model.<locals>.load\u001b[1;34m(module, state_dict, prefix)\u001b[0m\n\u001b[0;32m    567\u001b[0m \u001b[39mfor\u001b[39;00m name, child \u001b[39min\u001b[39;00m module\u001b[39m.\u001b[39m_modules\u001b[39m.\u001b[39mitems():\n\u001b[0;32m    568\u001b[0m     \u001b[39mif\u001b[39;00m child \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 569\u001b[0m         load(child, state_dict, prefix \u001b[39m+\u001b[39;49m name \u001b[39m+\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\modeling_utils.py:569\u001b[0m, in \u001b[0;36m_load_state_dict_into_model.<locals>.load\u001b[1;34m(module, state_dict, prefix)\u001b[0m\n\u001b[0;32m    567\u001b[0m \u001b[39mfor\u001b[39;00m name, child \u001b[39min\u001b[39;00m module\u001b[39m.\u001b[39m_modules\u001b[39m.\u001b[39mitems():\n\u001b[0;32m    568\u001b[0m     \u001b[39mif\u001b[39;00m child \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 569\u001b[0m         load(child, state_dict, prefix \u001b[39m+\u001b[39;49m name \u001b[39m+\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\modeling_utils.py:565\u001b[0m, in \u001b[0;36m_load_state_dict_into_model.<locals>.load\u001b[1;34m(module, state_dict, prefix)\u001b[0m\n\u001b[0;32m    563\u001b[0m                     module\u001b[39m.\u001b[39m_load_from_state_dict(\u001b[39m*\u001b[39margs)\n\u001b[0;32m    564\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 565\u001b[0m         module\u001b[39m.\u001b[39;49m_load_from_state_dict(\u001b[39m*\u001b[39;49margs)\n\u001b[0;32m    567\u001b[0m \u001b[39mfor\u001b[39;00m name, child \u001b[39min\u001b[39;00m module\u001b[39m.\u001b[39m_modules\u001b[39m.\u001b[39mitems():\n\u001b[0;32m    568\u001b[0m     \u001b[39mif\u001b[39;00m child \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:2040\u001b[0m, in \u001b[0;36mModule._load_from_state_dict\u001b[1;34m(self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs)\u001b[0m\n\u001b[0;32m   2038\u001b[0m                 \u001b[39msetattr\u001b[39m(\u001b[39mself\u001b[39m, name, input_param)\n\u001b[0;32m   2039\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 2040\u001b[0m             param\u001b[39m.\u001b[39;49mcopy_(input_param)\n\u001b[0;32m   2041\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m ex:\n\u001b[0;32m   2042\u001b[0m     error_msgs\u001b[39m.\u001b[39mappend(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mWhile copying the parameter named \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m   2043\u001b[0m                       \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mwhose dimensions in the model are \u001b[39m\u001b[39m{\u001b[39;00mparam\u001b[39m.\u001b[39msize()\u001b[39m}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m   2044\u001b[0m                       \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mwhose dimensions in the checkpoint are \u001b[39m\u001b[39m{\u001b[39;00minput_param\u001b[39m.\u001b[39msize()\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m   2045\u001b[0m                       \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39man exception occurred : \u001b[39m\u001b[39m{\u001b[39;00mex\u001b[39m.\u001b[39margs\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m   2046\u001b[0m                       )\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}],"source":["\n","\n","model = AutoModelForSequenceClassification.from_pretrained(\n","    checkpoint, num_labels=2)"]},{"cell_type":"markdown","metadata":{"id":"pwPBV42gbgh2"},"source":["You will notice that you get a warning after instantiating this pretrained model. This is because BERT has not been pretrained on classifying pairs of sentences, so the head of the pretrained model has been discarded and a new head suitable for sequence classification has been added instead. The warnings indicate that some weights were not used (the ones corresponding to the dropped pretraining head) and that some others were randomly initialized (the ones for the new head). It concludes by encouraging you to train the model, which is exactly what we are going to do now.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":6773,"status":"ok","timestamp":1655570887603,"user":{"displayName":"Ifeanyi Akawi","userId":"10822053801368667109"},"user_tz":-60},"id":"L9o69w-IbADb"},"outputs":[{"name":"stderr","output_type":"stream","text":["You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"]},{"ename":"KeyError","evalue":"'Category'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[1;32mc:\\Users\\Ben\\Documents\\GitHub\\Email-classification-with-bert\\train_model.ipynb Cell 25\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Ben/Documents/GitHub/Email-classification-with-bert/train_model.ipynb#X42sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m outputs \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39mmodel(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39minputs)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Ben/Documents/GitHub/Email-classification-with-bert/train_model.ipynb#X42sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m logits \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mlogits\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Ben/Documents/GitHub/Email-classification-with-bert/train_model.ipynb#X42sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m labels \u001b[39m=\u001b[39m inputs[\u001b[39m\"\u001b[39;49m\u001b[39mCategory\u001b[39;49m\u001b[39m\"\u001b[39;49m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Ben/Documents/GitHub/Email-classification-with-bert/train_model.ipynb#X42sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_function(logits, labels)  \u001b[39m# Compute the loss\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Ben/Documents/GitHub/Email-classification-with-bert/train_model.ipynb#X42sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n","\u001b[1;31mKeyError\u001b[0m: 'Category'"]}],"source":["\n","\n","# define trainer object\n","trainer = Trainer(\n","    model,\n","    training_args,\n","    train_dataset=tokenized_datasets,\n","    data_collator=data_collator,\n","    compute_metrics=compute_metrics,\n","    tokenizer=tokenizer,\n",")\n","\n"]},{"cell_type":"markdown","metadata":{"id":"1rMR-jXpdMXu"},"source":["To fine-tune the model on our dataset, we just have to call the train() method of our Trainer:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":373},"executionInfo":{"elapsed":3466,"status":"ok","timestamp":1655570890961,"user":{"displayName":"Ifeanyi Akawi","userId":"10822053801368667109"},"user_tz":-60},"id":"_T110yLucf_o","outputId":"45866c2d-396b-4659-d9f7-61bce14d2223"},"outputs":[{"name":"stderr","output_type":"stream","text":["  0%|          | 0/140 [19:41<?, ?it/s]\n","  0%|          | 0/140 [15:58<?, ?it/s]\n","  0%|          | 0/140 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"]},{"ename":"ValueError","evalue":"The model did not return a loss from the inputs, only the following keys: logits. For reference, the inputs it received are input_ids,token_type_ids,attention_mask.","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[1;32mc:\\Users\\Ben\\Documents\\GitHub\\Email-classification-with-bert\\train_model.ipynb Cell 27\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Ben/Documents/GitHub/Email-classification-with-bert/train_model.ipynb#X44sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# train the model\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Ben/Documents/GitHub/Email-classification-with-bert/train_model.ipynb#X44sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\trainer.py:1591\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1589\u001b[0m         hf_hub_utils\u001b[39m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1590\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1591\u001b[0m     \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   1592\u001b[0m         args\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m   1593\u001b[0m         resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[0;32m   1594\u001b[0m         trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[0;32m   1595\u001b[0m         ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[0;32m   1596\u001b[0m     )\n","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\trainer.py:1892\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1889\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler\u001b[39m.\u001b[39mon_step_begin(args, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol)\n\u001b[0;32m   1891\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39maccumulate(model):\n\u001b[1;32m-> 1892\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs)\n\u001b[0;32m   1894\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   1895\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   1896\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[0;32m   1897\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   1898\u001b[0m ):\n\u001b[0;32m   1899\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   1900\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\trainer.py:2776\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   2773\u001b[0m     \u001b[39mreturn\u001b[39;00m loss_mb\u001b[39m.\u001b[39mreduce_mean()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m   2775\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_loss_context_manager():\n\u001b[1;32m-> 2776\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_loss(model, inputs)\n\u001b[0;32m   2778\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mn_gpu \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   2779\u001b[0m     loss \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mmean()  \u001b[39m# mean() to average on multi-gpu parallel training\u001b[39;00m\n","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\trainer.py:2818\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[1;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[0;32m   2816\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   2817\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(outputs, \u001b[39mdict\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m outputs:\n\u001b[1;32m-> 2818\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   2819\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mThe model did not return a loss from the inputs, only the following keys: \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2820\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m,\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(outputs\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m. For reference, the inputs it received are \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m,\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(inputs\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2821\u001b[0m         )\n\u001b[0;32m   2822\u001b[0m     \u001b[39m# We don't use .loss here since the model may return tuples instead of ModelOutput.\u001b[39;00m\n\u001b[0;32m   2823\u001b[0m     loss \u001b[39m=\u001b[39m outputs[\u001b[39m\"\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(outputs, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m outputs[\u001b[39m0\u001b[39m]\n","\u001b[1;31mValueError\u001b[0m: The model did not return a loss from the inputs, only the following keys: logits. For reference, the inputs it received are input_ids,token_type_ids,attention_mask."]}],"source":["# train the model\n","trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1802,"status":"ok","timestamp":1655570892698,"user":{"displayName":"Ifeanyi Akawi","userId":"10822053801368667109"},"user_tz":-60},"id":"vZjIBUq7fUUQ","outputId":"d8e99315-5a81-4daf-9d11-324363ef2ce1"},"outputs":[],"source":["# save the trained model together with the tokenizer in a directory\n","trainer.save_model('custom_model')"]},{"cell_type":"markdown","metadata":{"id":"cWaWym99zcUg"},"source":["### Evaluation\n","\n","For this task, we will evaluate the model on the training set, given that the dataset is extremely small and could not be split into train-test sets\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":352},"executionInfo":{"elapsed":58,"status":"ok","timestamp":1655570892717,"user":{"displayName":"Ifeanyi Akawi","userId":"10822053801368667109"},"user_tz":-60},"id":"Nkl5dLmwgYST","outputId":"9bbe2b92-cd08-4298-950c-be4b768618ce"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 2/2 [00:01<00:00,  1.90it/s]"]},{"name":"stdout","output_type":"stream","text":["(11, 2) (11,) \n","\n","PredictionOutput(predictions=array([[-1.9196216,  1.3245827],\n","       [-1.949448 ,  1.3339628],\n","       [ 1.7652345, -1.1058381],\n","       [-2.0256405,  1.362749 ],\n","       [-2.0248377,  1.3859997],\n","       [ 1.6743466, -1.0574694],\n","       [ 1.7554849, -1.0992212],\n","       [-2.0404794,  1.3920761],\n","       [-1.9952593,  1.3621099],\n","       [ 1.7081995, -1.0845065],\n","       [-2.0348525,  1.3852504]], dtype=float32), label_ids=array([1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1], dtype=int64), metrics={'test_loss': 0.04296018183231354, 'test_accuracy': 1.0, 'test_runtime': 1.6477, 'test_samples_per_second': 6.676, 'test_steps_per_second': 1.214})\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["predictions = trainer.predict(tokenized_datasets)\n","print(predictions.predictions.shape, predictions.label_ids.shape, '\\n')\n","print(predictions)"]},{"cell_type":"markdown","metadata":{"id":"hS1xBv2f3oM-"},"source":["The output of the `predict()` method is another named tuple with three fields: predictions, `label_ids`, and `metrics`. The metrics field now contains the loss on the dataset passed, some time metrics (how long it took to predict, in total and on average), and the accuracy of training\n","\n","As we can see, predictions is a two-dimensional array with shape 11 x 2 (11 being the number of elements in the dataset we used). Those are the logits for each element of the dataset we passed to `predict()`. To transform them into predictions that we can compare to our labels, we need to take the index with the maximum value on the second axis:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":45,"status":"ok","timestamp":1655570892720,"user":{"displayName":"Ifeanyi Akawi","userId":"10822053801368667109"},"user_tz":-60},"id":"tWrsESYHqtgm"},"outputs":[],"source":["preds = np.argmax(predictions.predictions, axis=-1)"]},{"cell_type":"markdown","metadata":{"id":"NxjkyUq-6VVA"},"source":["We can now compare those preds to the labels. To build our `compute_metric()` function, we will rely on the metrics from the 🤗 Datasets library. We can load the metrics associated with the MRPC dataset as easily as we loaded the dataset, this time with the `load_metric()` function. The object returned has a `compute()` method we can use to do the metric calculation. Wrapping everything together, we get our `compute_metrics_mrpc()` function:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1655570893158,"user":{"displayName":"Ifeanyi Akawi","userId":"10822053801368667109"},"user_tz":-60},"id":"xpiiHNIM882M"},"outputs":[],"source":["def compute_metrics_mrpc(eval_preds):\n","    metric = load_metric(\"glue\", \"mrpc\")\n","    logits, labels = eval_preds.predictions, eval_preds.label_ids\n","    predictions = np.argmax(logits, axis=-1)\n","    return metric.compute(predictions=predictions, references=labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":364,"status":"ok","timestamp":1655570893515,"user":{"displayName":"Ifeanyi Akawi","userId":"10822053801368667109"},"user_tz":-60},"id":"gGs9IY4m-G-I","outputId":"19c64b2f-8a60-4917-92e2-061864b3abcc"},"outputs":[{"name":"stderr","output_type":"stream","text":["Downloading builder script: 5.76kB [00:00, ?B/s]                       \n"]},{"data":{"text/plain":["{'accuracy': 1.0, 'f1': 1.0}"]},"execution_count":29,"metadata":{},"output_type":"execute_result"}],"source":["compute_metrics_mrpc(predictions)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":27,"status":"ok","timestamp":1655570893516,"user":{"displayName":"Ifeanyi Akawi","userId":"10822053801368667109"},"user_tz":-60},"id":"9Sn6-O-p-dDr","outputId":"b28619c3-deb4-463e-96f3-ff62f9abcbf5"},"outputs":[{"data":{"text/plain":["array([[-1.9196216,  1.3245827],\n","       [-1.949448 ,  1.3339628],\n","       [ 1.7652345, -1.1058381],\n","       [-2.0256405,  1.362749 ],\n","       [-2.0248377,  1.3859997],\n","       [ 1.6743466, -1.0574694],\n","       [ 1.7554849, -1.0992212],\n","       [-2.0404794,  1.3920761],\n","       [-1.9952593,  1.3621099],\n","       [ 1.7081995, -1.0845065],\n","       [-2.0348525,  1.3852504]], dtype=float32)"]},"execution_count":30,"metadata":{},"output_type":"execute_result"}],"source":["predictions.predictions"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22,"status":"ok","timestamp":1655570893518,"user":{"displayName":"Ifeanyi Akawi","userId":"10822053801368667109"},"user_tz":-60},"id":"KQH9c_mv-NeE","outputId":"c2ca3852-e68f-49cd-dee2-6571731daad8"},"outputs":[{"data":{"text/plain":["array([1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1], dtype=int64)"]},"execution_count":31,"metadata":{},"output_type":"execute_result"}],"source":["predictions.label_ids"]},{"cell_type":"markdown","metadata":{"id":"9FRTXK4zu14Q"},"source":["From the above, we see that the model has a perfect prediction on the data it was trained on. This is highly flawed and can be ascribed to overfitting, but since we have no test set to evaluate on given the size of the sample data, we can assume that for the model to overfit at 20 epochs, it actually did well in learning the training dataset.\n","\n","### Inference\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2223,"status":"ok","timestamp":1655573150063,"user":{"displayName":"Ifeanyi Akawi","userId":"10822053801368667109"},"user_tz":-60},"id":"LZI4lSsd_6Up","outputId":"00bd1179-48de-4dbb-c68d-7acfaae6fda8"},"outputs":[],"source":["# get the directory where the model was saved to\n","inf_model = AutoModelForSequenceClassification.from_pretrained('custom_model/')"]},{"cell_type":"markdown","metadata":{},"source":["I had to change the path of the custom model to 'custom_model/'\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":380,"status":"ok","timestamp":1655573338250,"user":{"displayName":"Ifeanyi Akawi","userId":"10822053801368667109"},"user_tz":-60},"id":"IB80F5BT2W5r","outputId":"f4e87cc7-c38b-40d7-d376-6899aee91d52"},"outputs":[],"source":["# load the tokenizer by pointing to the same directory as the pretrained model\n","inf_tokenizer = AutoTokenizer.from_pretrained('custom_model/')"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":373,"status":"ok","timestamp":1655575117312,"user":{"displayName":"Ifeanyi Akawi","userId":"10822053801368667109"},"user_tz":-60},"id":"2hCuCbqM23CI"},"outputs":[],"source":["# generate sequence for inference\n","sequences = ['I want to know if I should send your email',\n","             'I sent your email a long time ago']"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":809,"status":"ok","timestamp":1655574094394,"user":{"displayName":"Ifeanyi Akawi","userId":"10822053801368667109"},"user_tz":-60},"id":"vLVXPplE3xxl"},"outputs":[],"source":["# create a pipeline for inference\n","from transformers import pipeline\n","classifier = pipeline(task='text-classification',\n","                      model=inf_model, tokenizer=inf_tokenizer)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":407,"status":"ok","timestamp":1655575120730,"user":{"displayName":"Ifeanyi Akawi","userId":"10822053801368667109"},"user_tz":-60},"id":"UgGbs7b65FDU","outputId":"d05c6a45-a20a-4995-edca-62da3f48d494"},"outputs":[{"data":{"text/plain":["[{'label': 'LABEL_1', 'score': 0.9348370432853699},\n"," {'label': 'LABEL_0', 'score': 0.7933637499809265}]"]},"execution_count":38,"metadata":{},"output_type":"execute_result"}],"source":["classifier(sequences)"]},{"cell_type":"markdown","metadata":{"id":"EJxapWpg7hlG"},"source":["From the above output, we can confidently say the model is performing well on inference\n"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyMu8uMKYqRgHJuCZ6XQuSiq","collapsed_sections":[],"mount_file_id":"1pZMgEIulV3c4SiuhbryXmdfF9-YkUv86","name":"train_model.ipynb","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.6"},"widgets":{"application/vnd.jupyter.widget-state+json":{"00d76b58b434493abaea93735bd876d8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0d8302ca67134193a10ade69c44041bf":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_89e96f9fcfed418895ef4887072b134d","IPY_MODEL_c6b02c2d616f41af9ded54c7916709ee","IPY_MODEL_b88ce659f6f4451eb43bf7bd6610f0d6"],"layout":"IPY_MODEL_30b853bfac2d4eafafa5208e914aa40e"}},"10ef2c11a1754e708834372100e6eb5b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c1cf39a62d064404920f5dbc2175829e","placeholder":"​","style":"IPY_MODEL_d1d37f46d21e476b80319fb3143055ff","value":" 1/1 [00:00&lt;00:00, 17.49ba/s]"}},"19bf2c0fa8c347ec84635386dba59ab5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"24c73ce427214973bde836688b2700c7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"27549b8720c741199e42b55049ac1c1f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_823582762af34fa3b85ab758fc06d8a3","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_df7418fee5f34b0584500f63aaa480eb","value":1}},"30b853bfac2d4eafafa5208e914aa40e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"36c4eb53fb1f40f9bd232c59979d52db":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3c095e7707b04738952a5b4483b496b5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"459c48905efe477cb25c5eb8dc3c0571":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fe8e0db9a7ce4810974b58c0f7fb9ff4","placeholder":"​","style":"IPY_MODEL_840f26b2aa4640b4919625d9504df3f6","value":"Casting the dataset: 100%"}},"4a4884cabad94a83b0fac2c12b0574e5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3c095e7707b04738952a5b4483b496b5","placeholder":"​","style":"IPY_MODEL_24c73ce427214973bde836688b2700c7","value":" 1/1 [00:00&lt;00:00, 11.43ba/s]"}},"5fe39bf5c8d446dc8dc9aa86429b7423":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7202ebce10d943f5885631fc36418058":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"823582762af34fa3b85ab758fc06d8a3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"840f26b2aa4640b4919625d9504df3f6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"89e96f9fcfed418895ef4887072b134d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5fe39bf5c8d446dc8dc9aa86429b7423","placeholder":"​","style":"IPY_MODEL_bf060a5ee51b458aa6517a3bf503e59b","value":"Casting to class labels: 100%"}},"a2505a274ff243589f05bf088c1e0992":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a39c3c5ee7d24900a6c2fe359c4261d2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b7032e7ea083403d9abf038b48c328e5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b88ce659f6f4451eb43bf7bd6610f0d6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f9214a7b86334b92b3df3f3389cf154e","placeholder":"​","style":"IPY_MODEL_7202ebce10d943f5885631fc36418058","value":" 1/1 [00:00&lt;00:00, 24.78ba/s]"}},"bf060a5ee51b458aa6517a3bf503e59b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c1cf39a62d064404920f5dbc2175829e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c6b02c2d616f41af9ded54c7916709ee":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a2505a274ff243589f05bf088c1e0992","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_36c4eb53fb1f40f9bd232c59979d52db","value":1}},"c72399c3c7ab4781b423a60ba393b5e3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_19bf2c0fa8c347ec84635386dba59ab5","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a39c3c5ee7d24900a6c2fe359c4261d2","value":1}},"d1d37f46d21e476b80319fb3143055ff":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"df6ea8d5f4764b738c8539fdbf5f47cb":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"df7418fee5f34b0584500f63aaa480eb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f1b50327eed54763b244185ab211cacf":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f9214a7b86334b92b3df3f3389cf154e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f954762c305c4be9ae3fd2682e7efe53":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_459c48905efe477cb25c5eb8dc3c0571","IPY_MODEL_27549b8720c741199e42b55049ac1c1f","IPY_MODEL_10ef2c11a1754e708834372100e6eb5b"],"layout":"IPY_MODEL_b7032e7ea083403d9abf038b48c328e5"}},"fa5f86eee50443969a0cd33309bd7127":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_faed0052fe0c461095f078464b9dc55c","IPY_MODEL_c72399c3c7ab4781b423a60ba393b5e3","IPY_MODEL_4a4884cabad94a83b0fac2c12b0574e5"],"layout":"IPY_MODEL_df6ea8d5f4764b738c8539fdbf5f47cb"}},"faed0052fe0c461095f078464b9dc55c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f1b50327eed54763b244185ab211cacf","placeholder":"​","style":"IPY_MODEL_00d76b58b434493abaea93735bd876d8","value":"100%"}},"fe8e0db9a7ce4810974b58c0f7fb9ff4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"nbformat":4,"nbformat_minor":0}
